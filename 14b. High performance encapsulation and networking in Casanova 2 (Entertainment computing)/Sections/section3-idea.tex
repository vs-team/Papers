In this section we introduce the idea of a code transformation technique that changes encapsulated programs into semantically equivalent but more efficient implementations.

\subsection{Optimizing lookup}

In our running example, the main drawback of the encapsulated version is that each planet has to check all the fleets to see if they are close enough to move into the list of attacking fleets. An optimization can be achieved by maintaining an index \texttt{FleetIndex} in \texttt{Planet}, containing a list of those \texttt{Fleets} that satisfy the attacking property, i.e., being owned by a different player and close enough to the planet. When an enemy \texttt{Fleet} is close enough to a \texttt{Planet}, it is moved into \texttt{FleetIndex} by the \texttt{Route}, which stores a list of travelling fleets. When \texttt{FleetIndex} changes, it notifies \texttt{Planet}, so that \texttt{Planet} can update \texttt{AttackingFleets}.

A predicate is a conditional statement based on one or more fields of an object of a class $A$. We can generalize the aforementioned situation by saying that encapsulation suffers from loss of performance whenever an object $B$ needs to update one of its fields depending on a predicate. $B$ stores an index $I_{A}$ that is used to keep track of all possible objects of class $A$ satisfying the predicate. Any object of $A$ has a reference to $B$ and is tasked with updating the index $I_{A}$ of $B$. $B$ checks $I_{A}$ every time it needs to interact with the instances of $A$ satisfying the predicate.

\subsection{Optimizing temporal/local predicates}

If we take into consideration the fact that predicates in a simulation are defined on entities (potentially hundreds or thousands) that exhibit similar behaviours (ships, bullets, asteroids, etc.) \cite{ai_dithering}, we can expect that some predicates will exhibit some sort of \textit{temporal locality} on their values. We can group those predicates, and their respective block of code, and apply an optimization that (\textit{i}) keeps their code block inactive in a \textit{fast wake-up} (we use the term \textit{fast} because the implementation uses a dictionary) collection, and (\textit{ii}) activates only those blocks of which the predicate has changed. In general, this would yield a higher performance without asking developers to write the optimization code themselves.

\subsection{Casanova 2-level integration}

The process described above can be automated at the compiler level as code transformation, since the index creation and management always follows the same pattern, and thus the compiler itself can create and update the required data structures.
Casanova is a \textit{Domain Specific Language} oriented towards game development. A program in Casanova is a set of entities organized in a tree hierarchy, of which the root is marked as \textit{world}. Each entity contains a set of fields, a set of rules, and a constructor. An extensive description of the formal grammar and semantics of Casanova can be found in \cite{maggiore2013casanova}. Casanova 2 (which we use) is a recent iteration of the original Casanova, which does not introduce changes to syntax or semantics. The language ensures that variables are only changed through specific statements; this makes it possible for the Casanova 2 compiler to identify patterns in code that are suitable for optimization. The Casanova 2 compiler applies transformations to the code that preserve the program semantics and optimize the encapsulated implementation by creating and maintaining the required indices. This way the code written by the programmer will gain the benefits of readability and maintainability that encapsulated code brings, without suffering from loss of performance or the necessity to break encapsulation to manage the optimization of data structures. In the next section we present the compiler architecture and the transformation rules. 

The Casanova 2 compiler is written in F\# and offers a modular extensible architecture made of a series of distinct layers, each performing a transformation task. Transformations initially add new information by means of analysis and inference. After analysis the compiler starts removing information in preparation for the code-generation step. This is the synthetic\footnote{In this context we are using the term ``synthetic'' in opposition to ``analytic'' to underline that the compiler has two phases: one that analyses the source code, and one that produces the target code by exploiting the information gathered by the analysis phase.} part of the compiler. To let the compiler support our technique, we need to implement and add a new layer whose task is to provide the compiler with information regarding the dependencies (necessary to implement the idea above), which is then used during the code-generation step.