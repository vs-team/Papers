\section{Architectural overview of a compiler}
\label{sec:ch_background_compiler_architecture}
Compilers are software that read as input a program written in a programming language, called \textit{source language}, and translate it into an equivalent program expressed with another programming language, called \textit{target language}. Usually the target language is machine code, but this is not mandatory. A special kind of compilers are interpreted, that directly execute the program written in the source language rather than translating it into a target language. Some languages, like Java, use a hybrid approach, that is they compile the program into an intermediate language that is later interpreted by a \textit{virtual machine}. Another approach involves the translation into a target high-level language [...].

Although the architecture of a compiler may slightly vary depending on the specific implementation, the translation process usually consists of the following steps:

\begin{enumerate}
	\item \textbf{Lexical analysis:} this phase is performed by a module called \textit{lexer} that is able to process the text and identify the syntactical elements of the language, called \textit{tokens}.
	\item \textbf{Syntactical analysis:} this phase is performed by a module called \textit{parser}, that checks whether the program written in the source language is compliant to the formal syntax of the language. The parser is tightly coupled with the lexer, as it needs to identify the tokens of the language to correctly process the syntax rules. The parser outputs a representation of the program, called \textit{Abstract Syntax Tree}, for later use.
	\item \textbf{Type checking:} this phase is performed by the \textit{type checker} that uses the rules defined by a \textit{type system} to assign a property to the elements of the language called \textit{type}. The types are used to determine whether the abstractions of the language, in a program that is syntactically correct, are used in a meaningful way.
	\item \textbf{Code generation:} the code generation phase requires to choose one or more target languages to emit. In the latter case, the code generator must have a modular structure to allow to interchange the output language. For this reason this step is usually preceded by an \textit{intermediate code generation} step, that converts the source program into an intermediate representation close to the target language. This phase can later be followed by different kinds of code optimization phases.
\end{enumerate}

In what follows we extensively describe each module that was summarized above.

\section{Lexer}
\label{sec:ch_background_compiler_lexer}
As stated above, the lexer task is to recognize the \textit{words} or \textit{tokens} of the source language. In order to perform this task the token structure must be expressed in a formal way. Below we present such formalization and we describe the algorithm that actually recognizes the token.

Let us consider a finite alphabet $\Sigma$, a \textit{language} is a set of strings, intended as sequences of characters in $\Sigma$.

\begin{definition}
	A string in a language $L$ in the alphabet $\Sigma$ is a tuple of characters $\mathbf{a} \in \Sigma^{n}$.
\end{definition}

 A notable difference between  languages in this context and human-spoken languages is that, in the former, we do not associate a meaning to the words but we are only interested to define which words are part of the language and which are not. Regular expressions are a convenient formalization to define the structure of sets of strings:

\begin{definition}
	\label{def:ch_background_regexp}
	 The following are the possible ways to define regular expressions:
	\begin{itemize}[noitemsep]
		\item \textit{Empty:} The regular expression $\epsilon$ is a language containing only the empty string.
		\item \textit{Symbol:} $\forall a \in \Sigma$, $\mathbf{a}$ is a string containing the character $a$.
		\item \textit{Alternation:} Given two regular expressions $M$ and $N$, a string in the language of $M | N$, called alternation, is the sets of strings in the language of $M$ or $N$.
		\item \textit{Concatenation:} Given two regular expressions $M$ and $N$, a string in the language of $M \cdot N$ is the language of strings $\mathbf{\alpha \cdot \beta}$ such as $\mathbf{\alpha} \in M$ and $\mathbf{\alpha} \in N$.
		\item \textit{Repetition:} Given a regular expression $M$, its Kleene Closure $M^{*}$ is formed by the concatenation of zero or more strings in the language $M$.
	\end{itemize}
\end{definition}

The regular expressions defined in Definition \ref{def:ch_background_regexp} can be combined to define tokens in a language.

Regular expressions can be processed by using a finite state automaton. Informally a finite state automaton is made of a finite set of states, an alphabet $\Sigma$ of which it is able to process the symbols, and a set of symbol-labelled edges that connect two states and define how to transition from one state to another. Automata can be divided into two categorise: \textit{non-deterministic finite state automata (NFA)} and \textit{deterministic finite state automata (DFA)}. Formally we have the following definitions:

\begin{definition}
	A non-deterministic finite state automaton (NFA) is made of:
	
	\begin{itemize}[noitemsep]
		\item A finite set of states S.
		\item An alphabet $\Sigma$ of input symbols.
		\item A state $s_{0} \in S$ that is the starting state of the automaton.
		\item A set of states $F \subset S$ called final or accepting states.
		\item A set of transitions $\mathcal{T} \subseteq S \times (\Sigma \cup \lbrace \epsilon \rbrace) \times S$.
	\end{itemize}
\end{definition}

\begin{definition}
	A deterministic finite state automaton (DFA) is a NFA where the transition is a function, i.e.
	\begin{equation*}
		\begin{array}{l}
			\tau : S \times \Sigma \rightarrow S\\
			\tau(s_{i},c) = s_{j}
		\end{array}
	\end{equation*}
	and $\nexists \; \tau(s,c_{i}),\tau(s,c_{j}) \; | \; c_{i} = c_{j} \; \forall i,j$.
\end{definition}

Informally, in NFA's there might be two transitions from the same state that can process the same symbol, while in DFA's for the same state there exists one and only one transition able to process a symbol and no transition processes the empty string. Regular expressions can be converted in NFA by using translation rules. The formalization of the algorithm can be found in \cite{mcnaughton1960regular}, here we just show an informal overview for brevity.

\subsection{Finite state automata for regular expressions}
\label{subsec:ch_background_automata}
In this section we present an informal overview of the translation rules for regular expressions into NFA's, and an algorithm to convert an NFA into a DFA.

\paragraph{Conversion for Symbols}
A regular expression containing just one symbol $a \in \Sigma$ can be converted by creating a transition $\tau(s_{i},a) = s_{j}$.

\paragraph{Conversion for concatenation}
The conversion for concatenation is recursive: the base case of the recursion is the symbol conversion. The conversion of a concatenation of $n$ symbols $a_{1}a_{2}, ..., a_{n}$ is obtained by adding a transition from the last state of the conversion for the first $n - 1$ symbols into a new state through a transition processing the n-th symbol, $\tau(s_{n - 1},a_{n}) = s_{n}$.

\paragraph{Conversion for alternation}
The alternation $M | N$ is obtained by creating an automata with a $\epsilon$-transition into a new state, that we call $s_{\epsilon}$. From $s_{\epsilon}$ we recursively generate the automata for both $M$ and $N$. Both automata can finally reach the same state through an $\epsilon$-transition.

\paragraph{Conversion for Kleene closure}
The Kleene Closure $M^{*}$ is obtained by initially creating an $\epsilon$-transition into a state $s_{\epsilon}$. $s_{\epsilon}$ can recursively transition to the automaton for $M$, which in turn transitions through an $\epsilon$-transition to $s_{\epsilon}$.

\paragraph{Conversion for $\mathbf{M^{+}}$}
The regular expression $M^{+}$ contains the concatenation of one or more strings in $M$. This can be translated by translating $M \cdot M^{*}$.

\paragraph{Conversion for $\mathbf{M?}$}
The regular expression $M?$ is a shortcut for $M|\epsilon$, thus it can be translated by using the conversion rule for the alternation.

\subsection{Conversion of a NFA into a DFA}
As stated in Section \ref{sec:ch_background_compiler_lexer}, a NFA might have, for the same state, a set of transitions that process the same symbol (including the empty string since $\epsilon$-transitions are allowed). This means that a NFA must be able to guess which transition to follow when trying to process a token. This is not efficient to implement in a computer, thus it is better to use a DFA where there can be only one way of processing a symbol for a given state. An algorithm to automate such conversion exists and is presented in \cite{aho2007compilers} but there exists an algorithm to directly convert regular expressions into DFA's, as shown in \cite{aho1986compilers}. Below we present the algorithm to convert NFA's into DFA's.

The informal idea behind the algorithm is the following: since a DFA cannot contain $\epsilon$-transitions or transitions from one state into another containing the same symbols, we have to construct an automaton that skips the $\epsilon$-transitions and pre-calculates the calculation of the sets of states in advance. In order to do so, we need to be able to compute the \textit{closure} of a set of states. Informally the closure of a set of states $S$ is the sates that can be reached by one of the states of $S$ through an $\epsilon$-transition. The formal definition is given below:

\begin{definition}
	The closure $\mathcal{C}(S)$ of a set of states $S$ is defined as
	\begin{itemize}[noitemsep]
		\item 
				$\displaystyle \mathcal{C}(S) = S \cup \left(\bigcup_{s \in T} \tau(s,\epsilon)	\right)$
		\item if $\exists \; \mathcal{C}'(S) \; | \; \mathcal{C}(S) \subseteq \mathcal{C}'(S) \Rightarrow \mathcal{C}'(S) = \mathcal{C}(S)$.
	\end{itemize}	
	
\end{definition}

\begin{algorithm}
	\caption{Closure of $S$}
	\label{alg:ch_background_closure}
	\begin{algorithmic}
		\State $T \gets S$
		\Repeat
			\State $T' \gets T$
			\State $T \gets \cup \left( \bigcup_{s \in  T'}\tau(s,\epsilon) \right)$
		\Until {$T = T'$}
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:ch_background_closure} computes the closure of a set of states. Note that the algorithm termination is granted because we are considering finite-state automata.

At this point we can build the set of all possible states reachable by consuming a specific character. We call this set \textit{edge} of a set of states $d$. 

\begin{definition}
	Let $d$ be a set of states, then the \textit{edge} of $d$ is defined as
	\begin{equation*}
		\mathcal{E}(d,c) = \mathcal{C}\left(\bigcup_{s \in d}\tau(s,c)\right)
	\end{equation*}
\end{definition}

\noindent
Now we can use the \textit{closure} and \textit{edge} to build the DFA from a NFA.

\begin{algorithm}
	\caption{NFA into DFA conversion}
	\label{alg:ch_background_dfa}
	\begin{algorithmic}
		\State $states[0] \gets \emptyset$
		\State $states[1] \gets \mathcal{C}(s_{1})$
		\State $p \gets 1$
		\State $j \gets 0$
		\While {$j \leq p$}
			\ForAll {$c \in \Sigma$}
				\State $e \gets \mathcal{E}(states[j],c)$
				\If {$\exists \; i \leq p \; | \; e = states[i]$}
					\State $trans[j,c] \gets i$
				\Else
					\State $p \gets p + 1$
					\State $states[p] \gets e$
					\State $trans[j,c] \gets p$
				\EndIf
			\EndFor
			\State $j \gets j + 1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\noindent
Algorithm \ref{alg:ch_background_dfa} performs the conversion into a DFA but we need to adjust it in order to mark the final states of the automaton. A state $d$ is final in the DFA if it is final if any of the states in $state[d]$ is final. In addition to marking final states, we must also keep track of what token is produced in that final state.

\section{Parser}
\label{sec:ch_background_parser}
Regular expressions are a concise declarative way to define the lexical structure of the terms of a language, but they are insufficient to describe its syntax, i.e. how to combine tokens together to  make ``sentences''. A compiler uses the parser module to check the syntactical structure of a program. As we will see more in depth below, the parser is tightly coupled with the lexer, which is used by it to recognize tokens. In order to present the structure of the parser, it is first necessary to introduce \textit{context-free grammars}.

As before we consider a language as a set of tuples of characters taken from a finite alphabet $\Sigma$. Informally, a context-free grammar is a set of productions of the form $symbol \rightarrow symbol_{1} \; symbol_{2} \; ... symbol_{n}$, where the left argument can be replaced by the sequence of symbols contained in the right argument. Some productions are \textit{terminal}, meaning that they cannot be replaced any longer, while the others are \textit{non-terminal}. Terminal symbols can only appear on the right side, while non-terminals can appear on both sides. Formally a context free grammar is defined as follows

\begin{definition}
	\label{def:ch_background_grammar}
	A \textit{context-free grammar} is made of the following elements:
	
	\begin{itemize}[noitemsep]
		\item A set of non-terminal symbols $N$.
		\item A finite set of terminal symbols $\Sigma$, called \textit{alphabet}.
		\item A non-terminal symbol $S \in N$ called \textit{starting symbol}.
		\item A set of productions $P$ in the form $N \rightarrow (N \cup \Sigma)^{*}$.
	\end{itemize}
\end{definition}

\noindent
Note that Definition \ref{def:ch_background_grammar} allows \textit{context-free} grammars to process also regular expression, thus context-free grammars are more expressive then regular expressions. In what follows we assume that the terminal symbols are treated as tokens with regular expressions that can be processed by a lexer, but in general a context-free grammar does not require a lexer DFA to process terminal symbols.

In order to check if a sentence is valid in the grammar defined for a language, we perform a process called \textit{derivation}: starting from the symbol $S$ of the grammar, we recursively replace non-terminal symbols with the right side of their production. The derivation can be done in different ways: we can start expanding the leftmost non-terminal in the production or the rightmost one. The result of the derivation usually generates a data structure called \textit{parse tree} or \textit{abstract syntax tree}, which connects a non-terminal symbol to the symbols obtained through the derivation; the leaves of the tree are terminal symbols.

\subsection{LR(k) parsers}
Simple grammars can be parsed by using \textit{left-to-right parse, leftmost-derivation, k-tokens lookahead}, meaning that the parser processes a symbol by performing a derivation starting from the leftmost symbol of the production, and looking at the first \textit{k} tokens of a string of the language. The weakness of this technique is that the parser must predict which production to use only knowing the first k tokens of the right side of the production. For instance, consider the two expression

\begin{equation*}
	\begin{array}{l}
		(15 * 3 + 4) - 6\\
		(15 * 3 + 4)
	\end{array}
\end{equation*}

\noindent
and the grammar

\begin{equation*}
	\begin{array}{l}
		S \rightarrow E \; eof\\
		E \rightarrow E + T\\
		E \rightarrow E - T\\
		E \rightarrow T * F\\
		E \rightarrow T / F\\
		E \rightarrow T\\
		T \rightarrow F\\
		F \rightarrow id\\
		F \rightarrow num\\
		F \rightarrow ( E )	
	\end{array}
\end{equation*}

\noindent
In the first case the parser should use the production $E \rightarrow E - T$ while in the second it should use the production $E \rightarrow T$. This grammar cannot be parsed by a LL(k) parser because it is not possible to decide which of the two productions must be used just by looking at the first k leftmost tokens. Indeed expressions of that form could have arbitrary length and the lookahead is, in general, insufficient. In general LL(k) grammars are context-free, but not all context-free grammars are LL(k), so such a parser is unable to parse all context-free grammars.

A more powerful parser is the \textit{left-to-right parse, rightmost-derivation, k-tokens lookahead} or LR(k). This parse maintains a \textit{stack} and an \textit{input} (which is the sentence to parse). The first k tokens of the input are the \textit{lookahead}. The parser uses the stack and the lookahead to perform two different actions:

\begin{itemize}
	\item \textit{Shift}: The parser moves the first input token to the top of the stack.
	\item \textit{Reduce}: The parser chooses a grammar production $N_{i} \rightarrow s_{1} \; s_{2} \; ... s_{j}$ and pop $s_{j}, s_{j - 1}, ... , s_{1}$ from the top of the stack. It then pushes $N_{i}$ at the top of the stack.
\end{itemize}

\noindent
The parser uses a DFA to know when to apply a shift action or a reduce action. The DFA is insufficient to process the input, as DFA's are not capable of processing context-free grammars, but it is applied to the stack. The DFA contains edges labelled by the symbols that can appear in the stack, while states contain one of the following actions:

\begin{itemize}[noitemsep]
	\item $s_{n}$: shift the symbol and go to state $n$.
	\item $g_{n}$: go to state $n$.
	\item $r_{k}$: reduce using the production $k$ in the grammar.
	\item $a$: accept, i.e. shift the end-of-file symbol.
	\item $error$: invalid state, meaning that the sentence is invalid in the grammar.
\end{itemize}

\noindent
The automaton is usually represented with a tabular structure, which is called \textit{parsing table}. The element $p_{i,s}$ in the table represents the transition from state $i$ when the symbol at the top of the stack is $s$.

In order to generate the parsing table (or equivalently the DFA for the parser) we need two support functions, one to generate the possible states the automaton can reach by using grammar productions, and one to generate the actions to advance past the current state. We introduce an additional notation to represent the situation where the parser has reached a certain position while deriving a production.

\begin{definition}
	An \textit{item} is any production in the form $N \rightarrow \alpha.X\beta$, meaning that the parser is at the position indicated by the dot where $X$ is a grammar symbol.
\end{definition}

At this point we are able to define the \textit{Closure} function, that adds more items to a set of items when the dot is before a non-terminal symbol, which is shown in Algorithm \ref{alg:ch_background_parser_closure}. Note that, for brevity, we present the version to generate a LR(0) parser, for a LR(1) parser a minor adjustment must be made.

\begin{algorithm}
	\caption{Closure function for a LR(0) parser}
	\label{alg:ch_background_parser_closure}
	\begin{algorithmic}
		\Function {Closure} {$I$}
			\Repeat
				\ForAll {$N \rightarrow \alpha.X\beta \; \in I$}
					\ForAll {$X \rightarrow \gamma$}
						\State $I \gets I \cup \left\lbrace X \rightarrow .\gamma \right\rbrace$
					\EndFor
				\EndFor
			\Until  {$I' \neq I$}
			\State \Return $I$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The algorithm starts with an initial set of items $I$ and adds all grammar productions that contain $X$ as left argument as items with the dot at the beginning of their right argument, meaning that the symbols of the production must still be completely parsed.
 
Now we need a function that, given a set of items, is able to advance the state of the parser past the symbol $X$. This is shown in Algorithm \ref{alg:ch_background_parser_goto}.

\begin{algorithm}
	\caption{Goto function for a LR(0) parser}
	\label{alg:ch_background_parser_goto}
	\begin{algorithmic}
		\Function {Goto} {$I, X$}
			\State $J \gets \emptyset$
			\ForAll {$N \rightarrow \alpha.X\beta \; \in I$}
				\State $J \gets J \cup \left\lbrace N \rightarrow \alpha X.\beta \right\rbrace$
			\EndFor
			\State \Return \Call {Closure} {$J$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The algorithm starts with a set of items and a symbol $X$ and creates a new set of items where the parser position has been moved past the symbol $X$. It then compute the closure of this new set of items a returns it.

We can now proceed to define the algorithm to generate the LR(0) parser, which is shown in Algorithm \ref{alg:ch_background_lr0_parser}. The initial state is made of all the productions where the left side is the starting symbol, which is equivalent to compute the closure of $S' \rightarrow .S \; eof$. It then proceeds to expand the set of states and the set of actions to perform. Note that we never compute GOTO$(I,eof)$ but we simply generate an \textit{accept} action. Now, for all actions in $E$ where $X$ is a terminal, we generate a shift action at position $(I,X)$, for all actions where $X$ is non-terminal we put a goto action at position $(I,X)$, and finally for a state containing an item $N_{k} \rightarrow \gamma.$ (the parser is at the end of the production) we generate a $r_{k}$ action at $(I,Y)$ for every token $Y$. 

In general parsing tables can be very large, for this reason it is usually wise to implement a variant of LR(k) parsers called LALR(k) parsers, where all states that contain the same actions but different lookaheads are merged into one, thus reducing the size of the parsing table. LR(1) and LALR(1) parsers are very common, since most of the programming languages can be defined by a LR(1) grammar. For instance, the popular family of parser generators \texttt{Yacc} produces LALR(1) parsers.

\begin{algorithm}
	\caption{LR(0) parser generation}
	\label{alg:ch_background_lr0_parser}
	\begin{algorithmic}
		\State $T \gets $ \Call {Closure} {$\left\lbrace S' \rightarrow .S \; eof \right\rbrace$}
		\State $E \gets \emptyset$
		\Repeat
			\State $T' \gets T$
			\State $E' \gets E$
			\ForAll {$I \in T$}
				\ForAll {$N \rightarrow \alpha.X\beta \; \in I$}
					\State $J \gets $ \Call {Goto} {$I,X$}
					\State $T \gets T \cup \lbrace J \rbrace$
					\State $E \gets E \cup \lbrace I \xrightarrow{X} J \rbrace$
				\EndFor
			\EndFor
		\Until {$E' = E \text{\textbf{ and }} T' = T$}
	\end{algorithmic}
\end{algorithm}

\subsection{Parser generators}


\subsection{Monadic parsers}
\label{sec:ch_background_parser_monad}
Monadic parsing is an alternative to traditional parsers, such as LR(k) and LALR(k) presented above. Monadic parsers have inferior performance with respect to LR(k) and LALR(k) \cite{hutton1998monadic} parsers but they are extensible, i.e. they do not rely on a limited set of combinators to describe the grammar of language as for parser generators. Monadic parsers were extensively explained in \cite{hutton1998monadic, wadler1995monads}, here we present a variation that can deal also with error handling. Before explaining how to implement a monadic parser, we introduce the concept of Monad:

\begin{definition}
	\label{def:ch_background_monad}
	A \textit{Monad} is a tern made of the following elements:
	\begin{itemize}[noitemsep]
		\item A type constructor $M$.
		\item A unary operation $Return \; :: \; a \rightarrow M \; a$.
		\item A binary operation $Bind \; :: \; M \; a \rightarrow (a \rightarrow M \; b) \rightarrow M \; b$. The bind can also be written by using the symbol $>>=$.
	\end{itemize}
	where both operations satisfy the following properties:
	\begin{itemize}[noitemsep]
		\item $a >>= return \equiv a$.
		\item $(a >>= f) >>= g \equiv a >>= (\lambda x.f x >>= g)$.
	\end{itemize}
\end{definition}
We now proceed to define a parser monad by defining (\textit{i}) the type constructor for the parser, (\textit{ii}) the unary operator, (\textit{iii}) the binary operator, and (\textit{iv}) parser combinators as an example of the extensibility of the parser monad. Note that below we provide an implementation in F\#, which does not have type classes as Haskell, so the parser monad does not use any type argument and directly defines the operators for this specific instance of monad.

\paragraph{Parser type constructor and monadic operations}

A parser is defined in literature as a function that takes as input a text and returns a list of pairs made of the parsing result and the rest of the text to process. The parsing result is usually the syntax tree generated by the parser. The result is a list because the same syntactical structure might be processed in different ways. By convention, an empty list denotes a parser failure. Here we propose a variation of this traditional implementation in order to provide a better error report.

In this alternative implementation, the parser is a function that takes as input the text to process, a \textit{parsing context} that might hold auxiliary information necessary for the parsing, the current position of the parser in the text, and returns either a tuple containing the parsing result, the text left to process, an updated context, and the updated position, or an error in case of a parser failure.

\begin{lstlisting}
type Parser<'a, 'ctxt> = { Parse : List<char> -> 'ctxt -> Position -> Either<'a * List<char> * 'ctxt * Position, Error>}

static member Make(p:List<char> -> 'ctxt -> Position -> Either<'a * List<char> * 'ctxt * Position, Error>) : Parser<'a,'ctxt> = { Parse = p }
\end{lstlisting}

The \textit{return} operation should take as input a generic value of type \texttt{'a} and return a \texttt{Parser<'a,'ctxt>}. The return simply creates the parser function for the given input:

\begin{lstlisting}
member this.Return(x:'a) : Parser<'a,'ctxt> =
  (fun buf ctxt pos -> First(x, buf, ctxt, pos)) |> Parser.Make
\end{lstlisting}

According to the Definition \ref{def:ch_background_monad}, the bind operator must take as input a \texttt{Parser<'a>}, a function \texttt{'a -> Parser<'b>} and return \texttt{Parser<'b>}. The bind generates a function that runs the input parser on the text. The result of the input parser can, according to its definition, contain a parsing result or an error in case of failure. The function generated by the bind must be able to handle these two situations: in case of a correct result the function creates a new parser using the parsing result and runs it on the remaining portion of the text, while in case of an error it simply outputs the error. In this way, when parsing fails, the error will be propagated ahead.

\begin{lstlisting}
member this.Bind(p:Parser<'a,'ctxt>, k:'a->Parser<'b,'ctxt>) : Parser<'b,'ctxt> =
(fun buf ctxt pos ->
  let all_res = p.Parse buf ctxt pos
  match all_res with
  | First p1res ->
      let res, restBuf, ctxt', pos' = p1res
      (k res).Parse restBuf ctxt' pos'
  | Second err -> Second err ) |> Parser.Make
\end{lstlisting}

\paragraph{Parser combinators}
With the parser monad implemented above, we can implement several parser combinators that can be used to define the grammar of a language. Here we show only a small glimpse of the possible combinators that can be implemented.\\\\
The first parser combinator that we present is the \textit{choice}. The choice takes as input two parsers and runs the first. If the first parser succeeds than its result is returned, otherwise the second is run. If it succeeds its result is return, otherwise the whole parser outputs an error. This combinator is useful, for instance, when there might be two possible choices for a token in a statement. For instance, in either Java or C\# is possible to exchange the order of the access modifier and the static modifier in the method declaration, thus both \texttt{public static} or \texttt{static public} are valid combinations. This combinator would try to parse the declaration in the first way, and if it fails it will try also the second option. Of course if the syntax of both combinations is wrong the parser will fail completely. The code for the combinator is shown below:

\begin{lstlisting}
static member (++) (p1:Parser<'a,'ctxt>, p2:Parser<'a,'ctxt>) : Parser<'a,'ctxt> = 
  (fun buf ctxt p ->
  match p1.Parse buf ctxt p with
  | Second err1 ->
      match p2.Parse buf ctxt p with
      | Second err2 -> Second err2
      | p2res -> p2res
  | p1res -> p1res) |> Parser.Make
\end{lstlisting}

\noindent
A useful variation of this combinator, is the one that executes two parsers with different generic types and returns a \texttt{Either} data type, containing either the result of the first or the second.
\begin{lstlisting}
static   member (+) (p1:Parser<'a,'ctxt>, p2:Parser<'b,'ctxt>) : Parser<Either<'a,'b>,'ctxt> = 
  (fun buf ctxt p ->
   match p1.Parse buf ctxt p with
   | Second err1 ->
       match p2.Parse buf ctxt p with
       | Second err2 -> Second(err2)
       | First p2res -> 
           let res,restBuf,ctxt',pos = p2res 
           First(Second res, restBuf, ctxt', pos)
   | First p1res ->
       let res, restBuf, ctxt', pos = p1res
       First((First res), restBuf, ctxt', pos)) |> Parser.Make
\end{lstlisting}

\noindent
Other combinators are possible, but for brevity we have only shown two. It should appear clear how this approach is completely extensible with no limitations. Any combinator would take as input two parsers and define the type of the resulting parser. The implementation will contain the logic to combine two parsers together. For example, another parser combinator is the application of 0 or more times of the same parser.

To complete this discussion, we now show how to parse a specific character and a keyword. The parser for a character takes as input the text to process and the character to match. If the input text is empty of course the parser immediately fails because no character will ever be matched. Otherwise if the first character of the text matches the one provided then we return the matched character as result and the rest of the text to process, otherwise we output an error. The function also takes care of updating the position of the parser accordingly and to skip line breaks.

\begin{lstlisting}
let character(c:char) : Parser<char, 'ctxt> = 
  (fun buf ctxt (pos:Position) ->
   match buf : List<char> with
   | x::cs when x = c -> 
       let pos' = 
       if x = '\n' then 
         pos.NextLine 
       else 
         pos.NextCol
         First( c, cs, ctxt, pos')
   | _ -> 
      Second (Error(pos, sprintf "Expected character %A" c))) |> Parser.Make
\end{lstlisting}

\noindent
The word parser takes as input the text to process and the word to match. It then applies the character parser to the word until it has all been processed. In the code below the syntax \texttt{let! x = y} is a syntactical sugar for \texttt{y >>= fun x -> ...} in the fashion of Haskell \texttt{do} notation.

\begin{lstlisting}
let rec word (w:List<char>) : Parser<List<char>, 'ctxt> =
  p{
    match w with
    | x::xs ->
        let! c = character x
        let! cs = word xs
        return c::cs
    | [] -> 
        return []
  }
\end{lstlisting}
\section{Type systems and type checking}
\label{sec:ch_background_type_checking}

\section{Operational semantics}
\label{sec:ch_background_semantics}

\section{Meta-compilers}

\begin{itemize}[noitemsep]
	\item General overview
	\item META-languages overview.
	\item RML overview.
	\item Possibly other meta-compilers (?)
\end{itemize}

Describe what it is existing in literature.



