This chapter provides background information on compiler construction and the existing knowledge on meta-compiles. The goal of this chapter is dual: (\textit{i}) it provides the reader with sufficient information to understand the implementation choices done when developing Metacasanova, and (\textit{ii}) outlines the complexity of the process of designing and implementing a compiler, thus giving further motivation to this research work.

In Section \ref{sec:ch_background_compiler_architecture} we outline the general architecture of a compiler by giving a short descriptions of all its components and how they work. In Section \ref{sec:ch_background_compiler_lexer} we give a detailed explanation about \textit{regular expressions} necessary to define the ``words'' of a language, and the \textit{lexer}, showing how to implement one. In Section \ref{sec:ch_background_parser} we introduce the notion of \textit{context-free grammars} and we show how to implement a parser able to process the grammatical rules of such grammar. In this chapter we present a parser generator for the language F\# that has been used for the implementation of Metacasanova, and then show an alternative to standard parsers in functional programming languages. We then explain how a type system and semantics of a language is expressed, and finally we introduce the concept of metaprogramming and we show examples using metaprogramming in the abstractions provided by a general purpose language (C++), and with existing dedicated metacompilers. As a side note, we want to point out that definitions below are mainly reformulations of what is presented in \cite{appel2002javacompiler} and \cite{aho2007compilers}.

\section{Architectural overview of a compiler}
\label{sec:ch_background_compiler_architecture}
Compilers are software that read as input a program written in a programming language, called \textit{source language}, and translate it into an equivalent program expressed with another programming language, called \textit{target language}. Usually the target language is machine code, but this is not mandatory. A special kind of compilers are interpreters, that directly execute the program written in the source language rather than translating it into a target language. Some languages, like Java, use a hybrid approach, that is they compile the program into an intermediate language that is later interpreted by a \textit{virtual machine}. Another approach involves the translation into a target high-level language.

Although the architecture of a compiler may slightly vary depending on the specific implementation, the translation process usually consists of the following steps:

\begin{enumerate}
	\item \textbf{Lexical analysis:} this phase is performed by a module called \textit{lexer} that is able to process the text and identify the syntactical elements of the language, called \textit{tokens}.
	\item \textbf{Syntactical analysis:} this phase is performed by a module called \textit{parser}, that checks whether the program written in the source language is compliant to the formal syntax of the language. The parser is tightly coupled with the lexer, as it needs to identify the tokens of the language to correctly process the syntax rules. The parser outputs a representation of the program, called \textit{Abstract Syntax Tree}, for later use.
	\item \textbf{Type checking:} this phase is performed by the \textit{type checker} that uses the rules defined by a \textit{type system} to assign a property to the elements of the language called \textit{type}. The types are used to determine whether the abstractions of the language, in a program that is syntactically correct, are used in a meaningful way.
	\item \textbf{Code generation:} the code generation phase requires to choose one or more target languages to emit. In the latter case, the code generator must have a modular structure to allow to interchange the output language. For this reason this step is usually preceded by an \textit{intermediate code generation} step, that converts the source program into an intermediate representation close to the target language. This phase can later be followed by different kinds of code optimization phases.
\end{enumerate}

In what follows we extensively describe each module that was summarized above.

\section{Lexer}
\label{sec:ch_background_compiler_lexer}
As stated above, the lexer task is to recognize the \textit{words} or \textit{tokens} of the source language. In order to perform this task the token structure must be expressed in a formal way. Below we present such formalization and we describe the algorithm that actually recognizes the token.

Let us consider a finite alphabet $\Sigma$, a \textit{language} is a set of strings, intended as sequences of characters in $\Sigma$.

\begin{definition}
	A string in a language $L$ in the alphabet $\Sigma$ is a tuple of characters $\mathbf{a} \in \Sigma^{n}$.
\end{definition}

 A notable difference between  languages in this context and human-spoken languages is that, in the former, we do not associate a meaning to the words but we are only interested to define which words are part of the language and which are not. Regular expressions are a convenient formalization to define the structure of sets of strings:

\begin{definition}
	\label{def:ch_background_regexp}
	 The following are the possible ways to define regular expressions \cite{appel2002javacompiler}:
	\begin{itemize}[noitemsep]
		\item \textit{Empty:} The regular expression $\epsilon$ is a language containing only the empty string.
		\item \textit{Symbol:} $\forall a \in \Sigma$, $\mathbf{a}$ is a string containing the character $a$.
		\item \textit{Alternation:} Given two regular expressions $M$ and $N$, a string in the language of $M | N$, called alternation, is the sets of strings in the language of $M$ or $N$.
		\item \textit{Concatenation:} Given two regular expressions $M$ and $N$, a string in the language of $M \cdot N$ is the language of strings $\mathbf{\alpha \cdot \beta}$ such as $\mathbf{\alpha} \in M$ and $\mathbf{\alpha} \in N$.
		\item \textit{Repetition:} Given a regular expression $M$, its Kleene Closure $M^{*}$ is formed by the concatenation of zero or more strings in the language $M$.
	\end{itemize}
\end{definition}

The regular expressions defined in Definition \ref{def:ch_background_regexp} can be combined to define tokens in a language.

Regular expressions can be processed by using a finite state automaton. Informally a finite state automaton is made of a finite set of states, an alphabet $\Sigma$ of which it is able to process the symbols, and a set of symbol-labelled edges that connect two states and define how to transition from one state to another. Automata can be divided into two categorise: \textit{non-deterministic finite state automata (NFA)} and \textit{deterministic finite state automata (DFA)}. Formally we have the following definitions:

\begin{definition}
	A non-deterministic finite state automaton (NFA) is made of:
	
	\begin{itemize}[noitemsep]
		\item A finite set of states S.
		\item An alphabet $\Sigma$ of input symbols.
		\item A state $s_{0} \in S$ that is the starting state of the automaton.
		\item A set of states $F \subset S$ called final or accepting states.
		\item A set of transitions $\mathcal{T} \subseteq S \times (\Sigma \cup \lbrace \epsilon \rbrace) \times S$.
	\end{itemize}
\end{definition}

\begin{definition}
	A deterministic finite state automaton (DFA) is a NFA where the transition is a function, i.e.
	\begin{equation*}
		\begin{array}{l}
			\tau : S \times \Sigma \rightarrow S\\
			\tau(s_{i},c) = s_{j}
		\end{array}
	\end{equation*}
	and $\nexists \; \tau(s,c_{i}),\tau(s,c_{j}) \; | \; c_{i} = c_{j} \; \forall i,j$.
\end{definition}

Informally, in NFA's there might be two transitions from the same state that can process the same symbol, while in DFA's for the same state there exists one and only one transition able to process a symbol and no transition processes the empty string. Regular expressions can be converted in NFA by using translation rules. The formalization of the algorithm can be found in \cite{mcnaughton1960regular}, here we just show an informal overview for brevity.

\subsection{Finite state automata for regular expressions}
\label{subsec:ch_background_automata}
In this section we present an informal overview of the translation rules for regular expressions into NFA's, and an algorithm to convert an NFA into a DFA.

\paragraph{Conversion for Symbols}
A regular expression containing just one symbol $a \in \Sigma$ can be converted by creating a transition $\tau(s_{i},a) = s_{j}$.

\paragraph{Conversion for concatenation}
The conversion for concatenation is recursive: the base case of the recursion is the symbol conversion. The conversion of a concatenation of $n$ symbols $a_{1}a_{2}, ..., a_{n}$ is obtained by adding a transition from the last state of the conversion for the first $n - 1$ symbols into a new state through a transition processing the n-th symbol, $\tau(s_{n - 1},a_{n}) = s_{n}$.

\paragraph{Conversion for alternation}
The alternation $M | N$ is obtained by creating an automata with a $\epsilon$-transition into a new state, that we call $s_{\epsilon}$. From $s_{\epsilon}$ we recursively generate the automata for both $M$ and $N$. Both automata can finally reach the same state through an $\epsilon$-transition.

\paragraph{Conversion for Kleene closure}
The Kleene Closure $M^{*}$ is obtained by initially creating an $\epsilon$-transition into a state $s_{\epsilon}$. $s_{\epsilon}$ can recursively transition to the automaton for $M$, which in turn transitions through an $\epsilon$-transition to $s_{\epsilon}$.

\paragraph{Conversion for $\mathbf{M^{+}}$}
The regular expression $M^{+}$ contains the concatenation of one or more strings in $M$. This can be translated by translating $M \cdot M^{*}$.

\paragraph{Conversion for $\mathbf{M?}$}
The regular expression $M?$ is a shortcut for $M|\epsilon$, thus it can be translated by using the conversion rule for the alternation.

\subsection{Conversion of a NFA into a DFA}
As stated in Section \ref{sec:ch_background_compiler_lexer}, a NFA might have, for the same state, a set of transitions that process the same symbol (including the empty string since $\epsilon$-transitions are allowed). This means that a NFA must be able to guess which transition to follow when trying to process a token. This is not efficient to implement in a computer, thus it is better to use a DFA where there can be only one way of processing a symbol for a given state. An algorithm to automate such conversion exists and is presented in \cite{aho2007compilers}. Another possible approach is an algorithm to directly convert regular expressions into DFA's, as shown in \cite{aho1986compilers}. Below we present the algorithm to convert NFA's into DFA's.

The informal idea behind the algorithm is the following: since a DFA cannot contain $\epsilon$-transitions or transitions from one state into another containing the same symbols, we have to construct an automaton that skips the $\epsilon$-transitions and pre-calculates the calculation of the sets of states in advance. In order to do so, we need to be able to compute the \textit{closure} of a set of states. Informally the closure of a set of states $S$ is the sates that can be reached by one of the states of $S$ through an $\epsilon$-transition. The formal definition is given below:

\begin{definition}
	The closure $\mathcal{C}(S)$ of a set of states $S$ is defined as
	\begin{itemize}[noitemsep]
		\item 
				$\displaystyle \mathcal{C}(S) = S \cup \left(\bigcup_{s \in T} \tau(s,\epsilon)	\right)$
		\item if $\exists \; \mathcal{C}'(S) \; | \; \mathcal{C}(S) \subseteq \mathcal{C}'(S) \Rightarrow \mathcal{C}'(S) = \mathcal{C}(S)$.
	\end{itemize}	
	
\end{definition}

\begin{algorithm}
	\caption{Closure of $S$}
	\label{alg:ch_background_closure}
	\begin{algorithmic}
		\State $T \gets S$
		\Repeat
			\State $T' \gets T$
			\State $T \gets \cup \left( \bigcup_{s \in  T'}\tau(s,\epsilon) \right)$
		\Until {$T = T'$}
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:ch_background_closure} computes the closure of a set of states. Note that the algorithm termination is granted because we are considering finite-state automata.

At this point we can build the set of all possible states reachable by consuming a specific character. We call this set \textit{edge} of a set of states $d$. 

\begin{definition}
	Let $d$ be a set of states, then the \textit{edge} of $d$ is defined as
	\begin{equation*}
		\mathcal{E}(d,c) = \mathcal{C}\left(\bigcup_{s \in d}\tau(s,c)\right)
	\end{equation*}
\end{definition}

\noindent
Now we can use the \textit{closure} and \textit{edge} to build the DFA from a NFA.

\begin{algorithm}
	\caption{NFA into DFA conversion}
	\label{alg:ch_background_dfa}
	\begin{algorithmic}
		\State $states[0] \gets \emptyset$
		\State $states[1] \gets \mathcal{C}(s_{1})$
		\State $p \gets 1$
		\State $j \gets 0$
		\While {$j \leq p$}
			\ForAll {$c \in \Sigma$}
				\State $e \gets \mathcal{E}(states[j],c)$
				\If {$\exists \; i \leq p \; | \; e = states[i]$}
					\State $trans[j,c] \gets i$
				\Else
					\State $p \gets p + 1$
					\State $states[p] \gets e$
					\State $trans[j,c] \gets p$
				\EndIf
			\EndFor
			\State $j \gets j + 1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\noindent
Algorithm \ref{alg:ch_background_dfa} performs the conversion into a DFA but we need to adjust it in order to mark the final states of the automaton. A state $d$ is final in the DFA if it is final if any of the states in $state[d]$ is final. In addition to marking final states, we must also keep track of what token is produced in that final state.

\section{Parser}
\label{sec:ch_background_parser}
Regular expressions are a concise declarative way to define the lexical structure of the terms of a language, but they are insufficient to describe its syntax, i.e. how to combine tokens together to  make ``sentences'' \cite{aho2007compilers, appel2002javacompiler}. For example, trying to define an arithmetic expression with chained sums would lead to the following (recursive) regular expression:

\begin{lstlisting}
expr = "(" expr "+" expr ")" | digits
\end{lstlisting}

Now we would need to replace the regular expression with itself, thus obtaining

\begin{lstlisting}
expr = "(" "(" expr "+" expr ")" | digits "+" "(" expr "+" expr ")" | digits ")" | digits
\end{lstlisting}

\noindent
It is easy to see that this substitution would never end, as the regular expression keeps growing at each replacement.

A compiler uses the parser module to check the syntactical structure of a program. As we will see more in depth below, the parser is tightly coupled with the lexer, which is used by it to recognize tokens. In order to present the structure of the parser, it is first necessary to introduce \textit{context-free grammars}.

As before we consider a language as a set of tuples of characters taken from a finite alphabet $\Sigma$. Informally, a context-free grammar is a set of productions of the form $symbol \rightarrow symbol_{1} \; symbol_{2} \; ... symbol_{n}$, where the left argument can be replaced by the sequence of symbols contained in the right argument. Some productions are \textit{terminal}, meaning that they cannot be replaced any longer, while the others are \textit{non-terminal}. Terminal symbols can only appear on the right side, while non-terminals can appear on both sides. Formally a context free grammar is defined as follows

\begin{definition}
	\label{def:ch_background_grammar}
	A \textit{context-free grammar} is made of the following elements:
	
	\begin{itemize}[noitemsep]
		\item A set of non-terminal symbols $N$.
		\item A finite set of terminal symbols $\Sigma$, called \textit{alphabet}.
		\item A non-terminal symbol $S \in N$ called \textit{starting symbol}.
		\item A set of productions $P$ in the form $N \rightarrow (N \cup \Sigma)^{*}$.
	\end{itemize}
\end{definition}

\noindent
Note that Definition \ref{def:ch_background_grammar} allows \textit{context-free} grammars to process also regular expressions, thus context-free grammars are more expressive than regular expressions. In what follows we assume that the terminal symbols are treated as tokens with regular expressions that can be processed by a lexer, but in general a context-free grammar does not require a lexer DFA to process terminal symbols.

In order to check if a sentence is valid in the grammar defined for a language, we perform a process called \textit{derivation}: starting from the symbol $S$ of the grammar, we recursively replace non-terminal symbols with the right side of their production. The derivation can be done in different ways: we can start expanding the leftmost non-terminal in the production or the rightmost one. The result of the derivation usually generates a data structure called \textit{parse tree} or \textit{abstract syntax tree}, which connects a non-terminal symbol to the symbols obtained through the derivation; the leaves of the tree are terminal symbols.

\subsection{LR(k) parsers}
Simple grammars can be parsed by using \textit{left-to-right parse, leftmost-\\derivation, k-tokens lookahead} (alse called LL(k) parsers), meaning that the parser processes a symbol by performing a derivation starting from the leftmost symbol of the production, and looking at the first \textit{k} tokens of a string of the language. The weakness of this technique is that the parser must predict which production to use only knowing the first k tokens of the right side of the production. For instance, consider the two expression

\begin{equation*}
	\begin{array}{l}
		(15 * 3 + 4) - 6\\
		(15 * 3 + 4)
	\end{array}
\end{equation*}

\noindent
and the grammar

\begin{equation*}
	\begin{array}{l}
		S \rightarrow E \; eof\\
		E \rightarrow E + T\\
		E \rightarrow E - T\\
		E \rightarrow T * F\\
		E \rightarrow T / F\\
		E \rightarrow T\\
		T \rightarrow F\\
		F \rightarrow id\\
		F \rightarrow num\\
		F \rightarrow ( E )	
	\end{array}
\end{equation*}

\noindent
In the first case the parser should use the production $E \rightarrow E - T$ while in the second it should use the production $E \rightarrow T$. This grammar cannot be parsed by a LL(k) parser because it is not possible to decide which of the two productions must be used just by looking at the first k leftmost tokens. Indeed expressions of that form could have arbitrary length and the lookahead is, in general, insufficient. In general LL(k) grammars are context-free, but not all context-free grammars are LL(k), so such a parser is unable to parse all context-free grammars.

A more powerful parser is the \textit{left-to-right parse, rightmost-derivation, k-tokens lookahead} or LR(k). This parse maintains a \textit{stack} and an \textit{input} (which is the sentence to parse). The first k tokens of the input are the \textit{lookahead}. The parser uses the stack and the lookahead to perform two different actions:

\begin{itemize}
	\item \textit{Shift}: The parser moves the first input token to the top of the stack.
	\item \textit{Reduce}: The parser chooses a grammar production $N_{i} \rightarrow s_{1} \; s_{2} \; ... s_{j}$ and pop $s_{j}, s_{j - 1}, ... , s_{1}$ from the top of the stack. It then pushes $N_{i}$ at the top of the stack.
\end{itemize}

\noindent
The parser uses a DFA to know when to apply a shift action or a reduce action. The DFA is insufficient to process the input, as DFA's are not capable of processing context-free grammars, but it is applied to the stack. The DFA contains edges labelled by the symbols that can appear in the stack, while states contain one of the following actions:

\begin{itemize}[noitemsep]
	\item $s_{n}$: shift the symbol and go to state $n$.
	\item $g_{n}$: go to state $n$.
	\item $r_{k}$: reduce using the production $k$ in the grammar.
	\item $a$: accept, i.e. shift the end-of-file symbol.
	\item $error$: invalid state, meaning that the sentence is invalid in the grammar.
\end{itemize}

\noindent
The automaton is usually represented with a tabular structure, which is called \textit{parsing table}. The element $p_{i,s}$ in the table represents the transition from state $i$ when the symbol at the top of the stack is $s$.

In order to generate the parsing table (or equivalently the DFA for the parser) we need two support functions, one to generate the possible states the automaton can reach by using grammar productions, and one to generate the actions to advance past the current state. We introduce an additional notation to represent the situation where the parser has reached a certain position while deriving a production.

\begin{definition}
	An \textit{item} is any production in the form $N \rightarrow \alpha.X\beta$, meaning that the parser is at the position indicated by the dot where $X$ is a grammar symbol.
\end{definition}

At this point we are able to define the \textit{Closure} function, that adds more items to a set of items when the dot is before a non-terminal symbol, which is shown in Algorithm \ref{alg:ch_background_parser_closure}. Note that, for brevity, we present the version to generate a LR(0) parser, for a LR(1) parser a minor adjustment must be made.

\begin{algorithm}
	\caption{Closure function for a LR(0) parser}
	\label{alg:ch_background_parser_closure}
	\begin{algorithmic}
		\Function {Closure} {$I$}
			\Repeat
				\ForAll {$N \rightarrow \alpha.X\beta \; \in I$}
					\ForAll {$X \rightarrow \gamma$}
						\State $I \gets I \cup \left\lbrace X \rightarrow .\gamma \right\rbrace$
					\EndFor
				\EndFor
			\Until  {$I' \neq I$}
			\State \Return $I$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The algorithm starts with an initial set of items $I$ and adds all grammar productions that contain $X$ as left argument as items with the dot at the beginning of their right argument, meaning that the symbols of the production must still be completely parsed.
 
Now we need a function that, given a set of items, is able to advance the state of the parser past the symbol $X$. This is shown in Algorithm \ref{alg:ch_background_parser_goto}.

\begin{algorithm}
	\caption{Goto function for a LR(0) parser}
	\label{alg:ch_background_parser_goto}
	\begin{algorithmic}
		\Function {Goto} {$I, X$}
			\State $J \gets \emptyset$
			\ForAll {$N \rightarrow \alpha.X\beta \; \in I$}
				\State $J \gets J \cup \left\lbrace N \rightarrow \alpha X.\beta \right\rbrace$
			\EndFor
			\State \Return \Call {Closure} {$J$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The algorithm starts with a set of items and a symbol $X$ and creates a new set of items where the parser position has been moved past the symbol $X$. It then compute the closure of this new set of items a returns it.

We can now proceed to define the algorithm to generate the LR(0) parser, which is shown in Algorithm \ref{alg:ch_background_lr0_parser}. The initial state is made of all the productions where the left side is the starting symbol, which is equivalent to compute the closure of $S' \rightarrow .S \; eof$. It then proceeds to expand the set of states and the set of actions to perform. Note that we never compute GOTO$(I,eof)$ but we simply generate an \textit{accept} action. Now, for all actions in $E$ where $X$ is a terminal, we generate a shift action at position $(I,X)$, for all actions where $X$ is non-terminal we put a goto action at position $(I,X)$, and finally for a state containing an item $N_{k} \rightarrow \gamma.$ (the parser is at the end of the production) we generate a $r_{k}$ action at $(I,Y)$ for every token $Y$. 

In general parsing tables can be very large, for this reason it is usually wise to implement a variant of LR(k) parsers called LALR(k) parsers, where all states that contain the same actions but different lookaheads are merged into one, thus reducing the size of the parsing table. LR(1) and LALR(1) parsers are very common, since most of the programming languages can be defined by a LR(1) grammar. For instance, the popular family of parser generators \texttt{Yacc} produces LALR(1) parsers.

\begin{algorithm}
	\caption{LR(0) parser generation}
	\label{alg:ch_background_lr0_parser}
	\begin{algorithmic}
		\State $T \gets $ \Call {Closure} {$\left\lbrace S' \rightarrow .S \; eof \right\rbrace$}
		\State $E \gets \emptyset$
		\Repeat
			\State $T' \gets T$
			\State $E' \gets E$
			\ForAll {$I \in T$}
				\ForAll {$N \rightarrow \alpha.X\beta \; \in I$}
					\State $J \gets $ \Call {Goto} {$I,X$}
					\State $T \gets T \cup \lbrace J \rbrace$
					\State $E \gets E \cup \lbrace I \xrightarrow{X} J \rbrace$
				\EndFor
			\EndFor
		\Until {$E' = E \text{\textbf{ and }} T' = T$}
	\end{algorithmic}
\end{algorithm}

\subsection{Parser generators}
\label{sec:ch_background_parser_generators}
The process of creating a parser can be automated by using a \textit{Parser Generator}. A Parser generator is a programming language that accepts the definition of the grammar of a language and generates a parser (and a lexer) for it. As programming languages generally have a LALR(1) grammar \cite{appel2002javacompiler}, most of parser generators produce a LALR(1) parser. Since in this research work we used F\# as a development language, in this section we present the F\# lexer and parser generators, belonging to the Yacc generator family, known ans \textit{FsLex} and \textit{FsYacc}.

\subsubsection{Definition of a lexer in FsLex}
FsLex allows to define the tokens with the regular expression syntax. Each FsLex program begins with a header, where the programmer can specify auxiliary modules and functions to use in the lexer. After the header, it is possible to specify relevant regular expressions that are used by the lexer to analyse the tokens with the standard let-binding syntax of F\# \cite{fsharpbinding}. The right argument of this binding is a regular expression, which can be composed with the combinators for regular expressions seen in Section \ref{subsec:ch_background_automata}. For example the following regular expression can define the syntax for variable names in a programming language:

\begin{lstlisting}
let simpleId = ['a'-'z' 'A'-'Z'] ['a'-'z' 'A'-'Z' '_' '0'-'9']+
\end{lstlisting}

Regular expression bindings can be used as alias in the lexer definition. A lexer definition is identified by the keyword \texttt{rule} for the binding. The right side of a lexer definition contains a call to the function \texttt{parse}, which tries to execute one of the rules specified below to parse a token. Each lexer rule generates a result which is a token data structure. Token data structures are specified at parser level (see below). For instance, the following is a lexer able to recognize comparison operators:

\begin{lstlisting}
rule comparisonOperators = parse
| "=" { Parser.EQUAL }
| ">" { Parser.GT }
| ">=" { Parser.GEQ }
| "<" { Parser.LT }
| "<=" { Parser.LEQ }
| "<>" { Parser.NEQ }
\end{lstlisting}

Note that, in order to provide useful information about the lexing phase, we might need to access, for instance, the position of the lexer (for error reporting), or to get the string read by the lexer (for example to generate literals when reading numbers or strings). This information is provided by the \textit{lexer buffer}, which is a data structure generated automatically by the parser generator. For example, if the token needs to store its row and column position in the file for error reporting, the definition above can be changed in this way (note the use of the header to define the function \texttt{range}):


\begin{lstlisting}
{
  module Lexer
  
  let range (lexbuf : LexBuffer<_>) = lexbuf.EndPos.Line + 1, lexbuf.EndPos.Column
}

rule comparisonOperators = parse
| ">" { Parser.GT (range lexbuf) }
| ">=" { Parser.GEQ (range lexbuf) }
| "<" { Parser.LT (range lexbuf) }
| "<=" { Parser.LEQ (range lexbuf) }
| "<>" { Parser.NEQ (range lexbuf) }
\end{lstlisting}

Another useful feature of FsLex is the capability of defining recursive lexers. Let us consider the case of skipping multi-line comments: usually such comments are delimited by a start and end symbol, and the comments spread across multiple lines. For example in \texttt{C++/Java/C\#} a multi-line comment is delimited by the symbols \texttt{/* */}. The lexer must detect the left delimiter of the multi-line comment, and then keep skipping all the symbols until it detects the right delimiter. This means that the lexer must call itself multiple times, using different lexing rules: one to detect the left delimiter, one to handle new lines or characters inside the comment, and one to detect the right delimiter. Furthermore, after handling the comment, the lexer must go back to processing the program normally. The following code shows how to implement such lexer:

\begin{lstlisting}
{
  module Lexer
  
  let newline (lexbuf : LexBuffer<_>) = lexbuf.EndPos <- lexbuf.EndPos.NextLine
  let range (lexbuf : LexBuffer<_>) = lexbuf.EndPos.Line + 1, lexbuf.EndPos.Column
}
let newline = ('\n' | '\r' '\n')

rule comment = parse
| "*/"    { programTokens lexbuf }
| newline            { newline lexbuf; comment lexbuf }
| _  { comment lexbuf }

and programTokens = parse
| "/*" { comment lexbuf }
...
//other token definitions
\end{lstlisting}
Note that \texttt{programTokens} calls \texttt{comment} when it detects the left delimiter of a multi-line comment. \texttt{comment} keeps calling itself until the right delimiter is detected, where it jumps back to \texttt{programTokens}.

\subsubsection{Definition of a parser in FsYacc}
FsYacc allows to define the grammar of a language in terms of productions of a context free grammar. As for the lexer, the parser definition starts with a header where the programmer can specify custom code and modules to use. The grammar defines terminal symbols as tokens, identified by the keyword \texttt{\%token}. A token specified the name to be used in the grammar productions, and a series of type parameters that are used to store data in a token. For example, the following tokens might be used in a parser for arithmetic expressions:

\begin{lstlisting}
%token PLUS MINUS MUL DIV LPAR RPAR
%token <double> NUMBER
\end{lstlisting}

\noindent
Whenever a terminal symbol is encountered during the parsing phase, the parser calls the lexer to generate the data structure for the token. The lexer tries to match the string provided by the parser by using one of its rule and, if it succeeds, it returns the appropriate token data structure. In this part of the grammar we must also specify the starting symbol. This symbol is defined through the keyword \texttt{\%start}. Since usually we want to generate an abstract syntax tree for the grammar (which must be manually defined), we can specify a return type generated by the parser with the keyword \texttt{\%type}. For an arithmetic expression this would be, for instance

\begin{lstlisting}
%start start
%type <Expr> start 
\end{lstlisting}

\noindent
In this section it is also possible to define the operators associativity and precedence, through the keywords \texttt{\%left}, \texttt{\%right}, and \texttt{\%nonassoc}. Terms defined in the same associativity line have the same precedence, and the precedence is ordered according to the line number, so if a term associativity is defined below another, it has higher precedence.

After the terminal symbol definitions, the grammar must specify productions. A production is defined in the following way:

\begin{lstlisting}
productionName:
| rule_1 { action_1 }
| rule_2 { action_2 }
...
| rule_n { action_n }
\end{lstlisting}

\noindent
Each action defines the code that the parser executes when that rule is matched. Usually this part is used to build the nodes of the syntax tree, but there is no restriction in what the action can perform, as long as it is valid F\# code. It is possible to access the result of evaluating a term in the production by using an index preceded by the symbol \texttt{\%}, where \texttt{\%1} refers to the first term in the right hand-side of the production. For example this code might be used to parse an arithmetic expression:

\begin{lstlisting}
%{
  open AST
%}

%token PLUS MINUS MUL DIV LPAR RPAR EOF
%token <float> NUMBER

%left PLUS MINUS
%left MUL DIV

%start start
%type <Expr> start

start : Expression EOF { %1 }

Expression:
| NUMBER { Number %1 }
| Expression PLUS Expression { Plus(%1,%3) }
| Expression MINUS Expression { Minus(%1,%3) }
| Expression MUL Expression { Mul(%1,%3) }
| Expression DIV Expression { Div(%1,%3) }
| LPAR Expression RPAR { Nested(%2) }
\end{lstlisting}

\begin{lstlisting}
module AST

type Expression =
| Number of float
| Plus of Expression * Expression
| Minus of Expression * Expression
| Mul of Expression * Expression
| Div of Expression * Expression
| Nested of Expression
\end{lstlisting}



\subsection{Monadic parsers}
\label{sec:ch_background_parser_monad}
Monadic parsing is an alternative to traditional parsers, such as LR(k) and LALR(k) presented above. Monadic parsers have inferior performance with respect to LR(k) and LALR(k) \cite{hutton1998monadic} parsers but they are extensible, i.e. they do not rely on a limited set of combinators to describe the grammar of a language as for parser generators. Monadic parsers were extensively explained in \cite{hutton1998monadic, wadler1995monads}, here we present a variation that can deal also with error handling. Before explaining how to implement a monadic parser, we introduce the concept of Monad:

\begin{definition}
	\label{def:ch_background_monad}
	A \textit{Monad} is a triplet made of the following elements:
	\begin{itemize}[noitemsep]
		\item A type constructor $M$.
		\item A unary operation $Return \; :: \; a \rightarrow M \; a$.
		\item A binary operation $Bind \; :: \; M \; a \rightarrow (a \rightarrow M \; b) \rightarrow M \; b$. The bind can also be written by using the symbol $>>=$.
	\end{itemize}
	where both operations satisfy the following properties:
	\begin{itemize}[noitemsep]
		\item $a >>= Return \equiv a$.
		\item $(a >>= f) >>= g \equiv a >>= (\lambda x.f x >>= g)$.
	\end{itemize}
\end{definition}

In other words, a monad is a functional design pattern that consists of a data container that provides two operations: one that takes an element whose type is compatible with the element of the container, and returns an instantiation of the container itself, and the other that defines a transformation between two data containers. Monads are a concept borrowed by functional programming language that comes from the much wider concept from category theory \cite{asperti1991categories, barr1985toposes, barr1990category, pierce1991basic}, whose usage is shown for example in \cite{moggi1991notions, peyton1993imperative,wadler1990comprehending, wadler1995monads}.
We now proceed to define a parser monad by defining (\textit{i}) the type constructor for the parser, (\textit{ii}) the unary operator, (\textit{iii}) the binary operator, and (\textit{iv}) parser combinators as an example of the extensibility of the parser monad. Note that below we provide an implementation in F\#, which does not have type classes as Haskell, so the parser monad does not use any type argument and directly defines the operators for this specific instance of monad.

\subsubsection{Parser type constructor and monadic operations}

A parser is defined in literature as a function that takes as input a text and returns a list of pairs made of the parsing result and the rest of the text to process. The parsing result is usually the syntax tree generated by the parser. The result is a list because the same syntactical structure might be processed in different ways. By convention, an empty list denotes a parser failure. Here we propose a variation of this traditional implementation in order to provide a better error report.

In this alternative implementation, the parser is a function that takes as input the text to process, a \textit{parsing context} that might hold auxiliary information necessary for the parsing, the current position of the parser in the text, and returns either a tuple containing the parsing result, the text left to process, an updated context, and the updated position, or an error in case of a parser failure.

\begin{lstlisting}
type Parser<'a, 'ctxt> = { Parse : List<char> -> 'ctxt -> Position -> Either<'a * List<char> * 'ctxt * Position, Error>}

static member Make(p:List<char> -> 'ctxt -> Position -> Either<'a * List<char> * 'ctxt * Position, Error>) : Parser<'a,'ctxt> = { Parse = p }
\end{lstlisting}

The \textit{return} operation should take as input a generic value of type \texttt{'a} and return a \texttt{Parser<'a,'ctxt>}. The return simply creates the parser function for the given input:

\begin{lstlisting}
member this.Return(x:'a) : Parser<'a,'ctxt> =
  (fun buf ctxt pos -> First(x, buf, ctxt, pos)) |> Parser.Make
\end{lstlisting}

According to the Definition \ref{def:ch_background_monad}, the bind operator must take as input a \texttt{Parser<'a>}, a function \texttt{'a -> Parser<'b>} and return \texttt{Parser<'b>}. The bind generates a function that runs the input parser on the text. The result of the input parser can, according to its definition, contain a parsing result or an error in case of failure. The function generated by the bind must be able to handle these two situations: in case of a correct result the function creates a new parser using the parsing result and runs it on the remaining portion of the text, while in case of an error it simply outputs the error. In this way, when parsing fails, the error will be propagated ahead.

\begin{lstlisting}
member this.Bind(p:Parser<'a,'ctxt>, k:'a->Parser<'b,'ctxt>) : Parser<'b,'ctxt> =
(fun buf ctxt pos ->
  let all_res = p.Parse buf ctxt pos
  match all_res with
  | First p1res ->
      let res, restBuf, ctxt', pos' = p1res
      (k res).Parse restBuf ctxt' pos'
  | Second err -> Second err ) |> Parser.Make
\end{lstlisting}

\subsubsection{Parser combinators}
With the parser monad implemented above, we can implement several parser combinators that can be used to define the grammar of a language. Here we show only a small glimpse of the possible combinators that can be implemented.\\\\
The first parser combinator that we present is the \textit{choice}. The choice takes as input two parsers and runs the first. If the first parser succeeds than its result is returned, otherwise the second is run. If it succeeds its result is return, otherwise the whole parser outputs an error. This combinator is useful, for instance, when there might be two possible choices for a token in a statement. For instance, in both Java and C\# is possible to exchange the order of the access modifier and the static modifier in the method declaration, thus both \texttt{public static} or \texttt{static public} are valid combinations. This combinator would try to parse the declaration in the first way, and if it fails it will try also the second option. Of course if the syntax of both combinations is wrong the parser will fail completely. The code for the combinator is shown below:

\begin{lstlisting}
static member (++) (p1:Parser<'a,'ctxt>, p2:Parser<'a,'ctxt>) : Parser<'a,'ctxt> = 
  (fun buf ctxt p ->
  match p1.Parse buf ctxt p with
  | Second err1 ->
      match p2.Parse buf ctxt p with
      | Second err2 -> Second err2
      | p2res -> p2res
  | p1res -> p1res) |> Parser.Make
\end{lstlisting}

\noindent
A useful variation of this combinator, is the one that executes two parsers with different generic types and returns a \texttt{Either} data type, containing either the result of the first or the second.
\begin{lstlisting}
static   member (+) (p1:Parser<'a,'ctxt>, p2:Parser<'b,'ctxt>) : Parser<Either<'a,'b>,'ctxt> = 
  (fun buf ctxt p ->
   match p1.Parse buf ctxt p with
   | Second err1 ->
       match p2.Parse buf ctxt p with
       | Second err2 -> Second(err2)
       | First p2res -> 
           let res,restBuf,ctxt',pos = p2res 
           First(Second res, restBuf, ctxt', pos)
   | First p1res ->
       let res, restBuf, ctxt', pos = p1res
       First((First res), restBuf, ctxt', pos)) |> Parser.Make
\end{lstlisting}

\noindent
Other combinators are possible, but for brevity we have only shown two. It should appear clear how this approach is completely extensible with no limitations. Any combinator would take as input two parsers and define the type of the resulting parser. The implementation will contain the logic to combine two parsers together. For example, another parser combinator is the application of zero or more times of the same parser.

To complete this discussion, we now show how to parse a specific character and a keyword. The parser for a character takes as input the text to process and the character to match. If the input text is empty of course the parser immediately fails because no character will ever be matched. Otherwise if the first character of the text matches the one provided then we return the matched character as result and the rest of the text to process, otherwise we output an error. The function also takes care of updating the position of the parser accordingly and to skip line breaks.

\begin{lstlisting}
let character(c:char) : Parser<char, 'ctxt> = 
  (fun buf ctxt (pos:Position) ->
   match buf : List<char> with
   | x::cs when x = c -> 
       let pos' = 
       if x = '\n' then 
         pos.NextLine 
       else 
         pos.NextCol
         First( c, cs, ctxt, pos')
   | _ -> 
      Second (Error(pos, sprintf "Expected character %A" c))) |> Parser.Make
\end{lstlisting}

\noindent
The word parser takes as input the text to process and the word to match. It then applies the character parser to the word until it has all been processed. In the code below the syntax \texttt{let! x = y} is a syntactical sugar for \texttt{y >>= fun x -> ...} in the fashion of Haskell \texttt{do} notation.

\begin{lstlisting}
let rec word (w:List<char>) : Parser<List<char>, 'ctxt> =
  p{
    match w with
    | x::xs ->
        let! c = character x
        let! cs = word xs
        return c::cs
    | [] -> 
        return []
  }
\end{lstlisting}

\section{Type systems and type checking}
\label{sec:ch_background_type_checking}
Being able to verify the correctness of a program is a crucial aspect of programming. When dealing with low-level languages, it is generally difficult to verify and grant the correctness of a program since a language such as assembly does not provide abstractions for the purpose. Modern high-level programming languages, on the other hand, generally provide a way to type their constructs. A type system is a syntactic method that assigns a property called \textit{type} to the constructs of a programming language, in order to prove that a program does not have certain unwanted behaviours \cite{pierce2002types}. Type systems are generally expressed in the form of inference rules \cite{cardelli1996type, pierce2002types}, made of a set of premises, that must be verified in order to assign to the language construct the type defined in the conclusion. An inference rule is a logical rule in the form:

\begin{mathpar}
\mprset{flushleft}
\inferrule*{premise_{1}\\\\premise_{2}\\\\ \vdots \\\\premise_{n}}
{conclusion}
\end{mathpar}

\noindent
where all the premises must be true in order to evaluate the conclusion. Usually the type rules make use of a \textit{typing environment}, which is an association between language constructs and types. For example the following rule defines the typing of an \texttt{if-then-else} and a \texttt{while-do} statement in an imperative language.

\begin{mathpar}
	\mprset{flushleft}
	\inferrule*{\Gamma \vdash c : bool \quad \Gamma \vdash t \quad \Gamma \vdash e}
	{\Gamma \vdash \text{if \textit{c} then \textit{t} else \textit{e}}}
\end{mathpar}

\begin{mathpar}
	\mprset{flushleft}
	\inferrule*{\Gamma \vdash c : bool \quad \Gamma \vdash w}
	{\Gamma \vdash \text{while \textit{c} do \textit{w}}}
\end{mathpar}

\noindent
In these rules $\Gamma$ is the environment. The type rule first evaluates the premises, which means that if the condition of the \texttt{if-then-else} has type \texttt{bool} and the evaluation of the \texttt{then} and \texttt{else} block succeeds, then the whole \texttt{if-then-else} is correctly typed. Analogously, for the \texttt{while-do}, if the condition has type \texttt{bool} and the evaluation of the while block is correctly typed, then the whole \texttt{while-do} is correctly typed. Note that control structures code blocks are usually not given a type, rather they are considered correct if all their statements are correctly typed. An equivalent way of expressing this is using a special type called \textit{unit} for constructs that do not return a value. This expedient is widely used in hybrid functional programming languages such as F\# or CamL. The equivalent rules for the construct above would be:

\begin{mathpar}
	\mprset{flushleft}
	\inferrule*{\Gamma \vdash c : bool \quad \Gamma \vdash t : unit \quad \Gamma \vdash e : unit}
	{\Gamma \vdash \text{if \textit{c} then \textit{t} else \textit{e} : unit}}
\end{mathpar}

\begin{mathpar}
	\mprset{flushleft}
	\inferrule*{\Gamma \vdash c : bool \quad \Gamma \vdash w : unit}
	{\Gamma \vdash \text{while \textit{c} do \textit{w} : unit}}
\end{mathpar}

Typing a construct of the language requires to evaluate its corresponding typing rule. Unlike for parsers, there exist no tools capable of automatically generating a type checker given the type rules definition, thus the behaviour of each type rule must be implemented in the host language in which the compiler is defined. Independently of the chosen language, the behaviour will always be the following : (\textit{i}) evaluate a premise, (\textit{ii}) if the evaluation of the premise fails then the construct fails the type check and an error is returned, (\textit{iii}) repeat step 1 and 2 until all the premises have been evaluated, and (\textit{iv}) assign the type to the construct that is defined in the rule conclusion.

During this process the compiler generates a data structure called \textit{symbol table}, which contains information about the type checking process and maintains the type environment.

At the end of the type check, the program is correct with respect to types. At this point, depending on the chosen target language, the compiler might discard or keep the information about the typing process. Usually when targeting a high-level programming language, the information about the types is kept because they are necessary during the code generation process in order to, for instance, generate the proper variable declaration statements. On the other hand, when targeting a low-level untyped programming language, such as assembly, the type information can be discarded.


\section{Semantics and code generation}
\label{sec:ch_background_semantics}
Semantics define how the language abstractions behave and can be expressed in different ways, for example with a term-rewriting system \cite{klop1992term}, reduction semantics \cite{FELLEISEN1992235} or with the operational semantics \cite{plotkin1981}. Below we provide a description of these three possible representations for the semantics:

\paragraph{Term-rewriting semantics}
Term-rewriting semantics define a set of rewriting rules that take as input a construct of the language and define how to rewrite it into another form. The rewriting process usually ends when a rewrite rule is replaced by itself. For example the \texttt{if-then-else} statements and \texttt{while-do} statements can be rewritten with these rules (the \texttt{;} symbol denotes a sequence of statements):

\begin{lstlisting}[mathescape = true]
if true then t else e ; k $\rightarrow$ t;k
if false then t else e ; k $\rightarrow$ e;k
while (c = true) do w ; k $\rightarrow$ w ; while (c) do w ; k
while (c = false) do w; k $\rightarrow$ k 
\end{lstlisting}

\paragraph{Reduction semantics}
Reduction semantics use a \textit{reduction context}, which is a program or a fragment of program with a \textit{hole} (denoted by the symbol $\Box$) as placeholder to mark where the next computational step is taking place. For example the \texttt{if-then-else} and \texttt{while-do} statements semantics can be represented in the following way\footnote{\texttt{skip} is a statement that simply skips to the next statement in a sequence of statements}:

\begin{lstlisting}[mathescape = true]
if $\Box$ then s1 else s2 ; k
if true then t else e  $\rightarrow$ t
if false then t else e $\rightarrow$ e
while $\Box$ do w
while true do w $\rightarrow$ w ; while $\Box$ do w
while false do w $\rightarrow$ skip
\end{lstlisting}


\paragraph{Operational semantics}
Operational semantics define the behaviour of language constructs in terms of logical rules similar to those used for type systems. For instance, the \texttt{if-then-else} and \texttt{while-do} semantics are expressed as
\begin{mathpar}
	\mprset{flushleft}
	\inferrule*
	{\langle c \rangle \Rightarrow \text{\texttt{true}}}
	{\langle \text{if \textit{c} then \textit{T} else \textit{E} ; \textit{k}} \rangle \Rightarrow \text{\textit{T} ; \textit{k}}}
\end{mathpar}

\begin{mathpar}
	\mprset{flushleft}
	\inferrule*
	{\langle c \rangle \Rightarrow \text{\texttt{false}}}
	{\langle \text{if \textit{c} then \textit{T} else \textit{E} ; \textit{k}} \rangle \Rightarrow \text{\textit{E} ; \textit{k}}}
\end{mathpar}


\begin{mathpar}
	\mprset{flushleft}
	\inferrule*
	{\langle c \rangle \Rightarrow \text{\texttt{true}}}
	{\langle \text{while \textit{c} do \textit{L} ; \textit{k}} \rangle \Rightarrow \text{\textit{L} ; while \textit{c} do \textit{L} ; \textit{k}}}
\end{mathpar}
\begin{mathpar}	
	\inferrule*
	{\langle c \rangle \Rightarrow \text{\texttt{false}}}
	{\langle \text{while \textit{c} do \textit{L} ; \textit{k}} \rangle \Rightarrow k }
\end{mathpar}

Regardless of the formal representation chosen for the semantics, this must be encoded in the abstractions of the target language during the code generation phase. When choosing a high-level target language encoding the operational semantics of a similar high-level language might be trivial, but generating for instance the code for a functional programming language into an imperative language might prove difficult. For this reason, the code generation step might be preceded by an intermediate code generation step. The intermediate language is usually a simple programming language close to the target language. Notable examples of this are the \textit{three-address code} \cite{aho2007compilers}, and the intermediate language used in the \textit{Glasgow Haskell Compiler} \cite{hall1993glasgow} and \textit{Utrecht Haskell Compiler} \cite{dijkstra2009architecture}.

\section{Metaprogramming and metacompilers}
\label{sec:ch_background_metaprogramming}
This section aims to provide the reader with sufficient information to understand the concept of metaprogramming. In this section we explain what metaprogramming is and present existing metacompilation approaches existing in scientific literature. We start by defining what metaprogramming is and we present techniques of metaprogramming in existing programming languages. We then proceed by presenting how different existing metacompilers work.

Metaprogramming is the process of writing computer programs with the ability to treat programs as their data \cite{czarnecki2000generative}. Metaprogramming takes as input a program written in a meta-language to define a programming language called \textit{object language}, a program written in the object language, and outputs executable code able to run the program. Metaprogramming can be achieved in two different ways: (\textit{i}) by using opportune language abstractions provided by a general-purpose programming language, or (\textit{ii}) using a dedicated metacompiler. In what follows we provide examples in both areas.

\subsection{Template metaprogramming}
\label{sec:ch_background_template_metaprogramming}
Template metaprogramming uses class templates to operate on numbers and types as data. In this section we provide examples in C++ templates, but other languages allow template metaprogramming, with notable examples being Lisp macros and Haskell templates \cite{sheard2002template}. The template language uses template recursion as loop construct and template specialization as decisional construct.

To better understand how this works, we will implement the factorial function with templates. It is well known that, by definition, the factorial of 0 is 1, while the factorial of a number $n$ is $n$ multiplied by the factorial of $n - 1$. Our template meta-program will thus contain two templates: one for the base case of the recursion and one for the recursive step. The base case of the recursion uses template specialization to stop the computation and immediately return 1:

\begin{lstlisting}
template<>
struct Factorial<0>
{
  enum { RET = 1 };
};
\end{lstlisting}

This template contains an enumeration type whose only value is 1. The recursive step will take as input a generic template parameter and recursively call the template definition.

\begin{lstlisting}
template<int n>
struct Factorial
{
  enum { RET = Factorial<n - 1>::RET * n }
};
\end{lstlisting}

\noindent
When the \texttt{Factorial} template is instantiated with a value different from 0, the non-specialized version is used by the C++ compiler. The enumeration case \texttt{RET} then gets the value of \texttt{RET} for the same template instantiated for \texttt{n - 1} and multiplied by \texttt{n}. The generation of templates and their enumeration cases will stop when the template instantiation will be invoked with \texttt{Factorial<0>}, which will use the specialized version. Note the use of the scope resolution operator to access the value of the enumeration case. This template can be used instantiating the template with an integer constant, for instance:

\begin{lstlisting}
int main()
{
  cout << Factorial<5>::RET << endl;
}
\end{lstlisting}

\noindent
Note that template instantiation is performed at compile-time, so the result of the factorial is actually inlined by the compiler every time the template is instantiated.

A more interesting example is about how to define recursive data structures. Let us consider the implementation of lists in a functional programming language:

\begin{lstlisting}
type List<'a> =
| Empty
| Cons of 'a * List<'a>
\end{lstlisting}

\noindent
where the list \texttt{[3,4,5,6]} can be built as \texttt{Cons(3,Cons\\(4,Cons(5,Cons(6,Empty))))}. This list representation can be defined with template metaprogramming by defining a template specialization for the empty list, and a non-specialized template for a non-empty list.

\begin{lstlisting}
struct NIL
{
  typedef NIL Head;
  typedef NIL Tail;
};

template<class T, class Tail_ = NIL>
struct Cons
{
  typedef T Head;
  typedef Tail_ Tail;
};
\end{lstlisting}

\noindent
Note that the assignment in the template definition specifies an optional template parameter, in the same way as optional method arguments. 

Now let us try to define a function to calculate the length of an arbitrary list. This function will have as a base case the empty list, for which it returns 0, otherwise it returns 1 plus the length of the tail. Again this can be implemented with a specialized template and a non-specialized template.

\begin{lstlisting}
template<class List>
struct Length
{
  static const unsigned int RET = Length<List::Tail>::RET + 1;
};

template<>
struct Length<NIL>
{
  static const unsigned int RET = 0;
};
\end{lstlisting}

\noindent
The first template recursively class the \texttt{Length} template with the type of the tail of the list, extracts the RET field and adds 1. The second template is a specialization created with the \texttt{NIL} type and immediately sets the field RET to 0.

In order to test this function we must create a template for a data type (which is a meta-data) that we want to store in the list. In this example we show how to test the function for a list of integers. First of all we must create a template for an integer:

\begin{lstlisting}
template<int n>
struct Int
{
  static const int Value = n;
};
\end{lstlisting}

\noindent
This is necessary in order to be able to store the values of the list elements. At this point, the list can be created by calling \texttt{Cons} and passing the \texttt{Int} data type as argument. Length can then be used with the type of the list that has been created and then we can access its \texttt{RET} field.

\begin{lstlisting}
typedef Cons<Int<3>, Cons<Int<4>, Cons<Int<5>, Cons<Int<6>>>>> testList;
cout << Length<testList>::RET << endl;
\end{lstlisting} 

Template metaprogramming complexity can grow exponentially, for example when we want to get the values of the list elements, to the point that a simple function as \texttt{nth} requires several templates. We omit the details here, but the reader can find additional information in Appendix \ref{app:template}.
%maybe put the code in the appendix
\subsection{Metacompilers}
Metacompilers are a special class of compilers used to implement other compilers. A metacompiler takes as input the definition of the syntax, semantics, and possibly the type system of the object language, a program written in the object language, and outputs executable code for it. Metacompilers are written either in a general purpose programming language or in their own meta-language through the process of self-hosting. Self-hosting compilation requires to write a prototypical version of the compiler in another language or an interpreter for it and then use it to compiler the implementation of a subsequent version. In this section we present four existing meta-compilers: (\textit{i}) META-II for historical reasons to show that research on meta-compilation had actually been made in early 1970's but the capability of early meta-compilers were limited, (\textit{ii}) RML that is based on natural semantics and, for some aspects, similar to Metacasanova, the meta-compiler that we describe in this work, (\textit{iii})Stratego, a meta-compiler that is based on term-rewriting semantics, to show an alternative approach to meta-compilers based on natural semantics, and (\textit{iv}) Rascal a meta-programming language used for source code analysis and transformation as well as for Domain-Specific language implementation.

\subsubsection{META-II}
META-II is one of the earliest metacompilers and, for this reason, quite limited in its capabilities. META-II allows to express the syntax of the object language and actions for the code generation. A meta-program in META-II is made of grammatical symbols, meta-variables, and equations that define the terms of the grammar. A symbol is written as a string surrounded by quotes and beginning with a period, a meta-variable is a string starting with a alphabetical character and followed by an arbitrary amount of alphanumerical characters, and an equation is a sequence of consecutive symbols or ids to indicate concatenation. Alternation is defined with the symbol \texttt{/}, which can be used together with the keyword \texttt{.EMPTY} to define alternation. For instance

\begin{lstlisting}
BOOLEAN = '.TRUE' / '.FALSE'
\end{lstlisting}

\noindent
defines boolean literals. The meta-language is able to recognize built-in symbols such as identifiers, denoted with \texttt{.ID}, strings represented by .STRING, and numbers represented by .NUMBER. These are to be intended as identifiers, strings, and numbers in the object language. For example the family of expressions:

\begin{lstlisting}
A
A + B
A + B * C
(A + B) * C
\end{lstlisting}

\noindent
can be encoded by the following equations in META-II

\begin{lstlisting}
EX3 = .ID / '(' EX1 ')',
EX2 = EX3 ('*' EX2 / .EMPTY),
EX1 = EX2 ('+' EX1 / .EMPTY)
\end{lstlisting}

\noindent
Sequences (as in the Kleene closure for regular expressions) can be expressed using the symbol \texttt{\$}. For example

\begin{lstlisting}
SEQA = $ 'A',
\end{lstlisting}

\noindent
represents a sequence containing the letter \texttt{A}.
META-II allows to associate actions to equations for the code generation. Each action generates assembly code for an interpreter called META-II machine, which is able to execute it. The action of code generation is marked with the keyword \texttt{.OUT}, for instance

\begin{lstlisting}
EX3 = .ID .OUT('LD ' *) / '(' EX1 ')'
\end{lstlisting}

\noindent
generates the literal output and the special symbol found in \texttt{EX3}.

META-II is a self-hosting compiler, i.e. it is implemented in META-II itself.

\subsubsection{RML}
RML \cite{pettersson1996compiler} (\textit{Relational Meta-Language}) uses a meta-language based on operational semantics. A program in RML consists of data definitions in a syntax similar to CamL variants \cite{minsky2013real} or F\# discriminated unions \cite{fsharunions}, and relations containing axioms and inference rules. An axiom is an inference rule without premises, while an inference rule generates an output if the premises correctly evaluate. For example the following snippet defines the data type for an arithmetic expression and a symbol table for the evaluation.

\begin{lstlisting}
datatype Expr =
| INT of int
| VAR of string
| ADD of Expr * Expr

type Env = (string * int) list
\end{lstlisting}

Axioms and inference rules are grouped together into a \textit{relation}. For example, the following relation can be used to evaluate an arithmetic expression:

\begin{lstlisting}
relation eval =
  axiom eval(env, INT i) => i
  
  rule
    lookup(env, x) => i
    ---------------------
    eval(env, VAR x) => i
    
  rule
    eval (env, left) => i1 &
    eval (env, right) => i2
    i1 + i1 => v
    ---------------------------------
    eval (env, ADD(left,right)) => v
\end{lstlisting}

\noindent
During the code generation phase, each relation is translated into a first-order logic representation, which consists of a series of \texttt{match} structures that check the structure of the arguments. For example the rule above would be translated into:\footnote{We use a prefix notation in Lisp style}

\begin{lstlisting}
(and (match [(arg1 env)
             (arg2 ADD(left,right))]))
(and (call eval [env left] [result1]))
(and match [result1 i1])
(and (call eval [env right] [result2]))
(and match [result2 i2])
(and call [i1 + i2] [result3])
(and match [result3 v])
(return v)
\end{lstlisting}

\noindent
This code is later translated into a continuation-passing style form, which is later generated as C code. The compiler performs heavy optimization on tail calls generated code through the use of a technique called \textit{dispatching switches}.

\subsubsection{Stratego}
Stratego \cite{bravenboer2008stratego} is a metacompiler that uses a term-rewriting semantics as meta-language to define its programs. A stratego program consists of a series of terms in the form

\begin{equation*}
t := c(t_{1},t_{2},...,t_{n})
\end{equation*}

\noindent
where $c$ is a constructor that accepts $n$ other terms as arguments. The syntax of Stratego has been enriched with additional syntax to handle ``traditional'' data structures, such as string, integer, float, constants, and lists:

\begin{equation*}
pt := s \; | \; i \; | \; | f \; | \; [t_{1},t_{2},...,t_{n}] \; | \; (t_{1},t_{2},...,t_{n}) \; | \; c(t_{1},t_{2},...,t_{n})
\end{equation*}

\noindent
Terms can be extended with a list of annotations that are terms themselves:

\begin{equation*}
t := pt \; | \; pt\left\lbrace t_{1},t_{2},...,t_{n} \right\rbrace
\end{equation*}

\noindent
Stratego requires that the meta-program specifies the signature of term constructors. For example simple arithmetic expressions can be defined as

\begin{lstlisting}
signature
  sorts Id Expr
  constructors
    Var : Id -> Exp
    Plus : Exp * Exp -> Exp
    
\end{lstlisting}

\noindent
Note that Stratego is an untyped language, so types are not statically checked and the compiler only checks that constructors are declared and have the correct arity.

Rewrite rules define how terms are evaluated, for example the following is a rewrite rule to evaluate a binary operator in an arithmetic expression:

\begin{lstlisting}
EvalBinOp : Plus(Int(i), Int(j)) -> Int(k) where k := <add>(i, j)
\end{lstlisting}

\noindent
Note that rewrite rules support conditionals, i.e. in the rule above we are able to specify that \texttt{k} is the result of adding the numbers \texttt{i} and \texttt{j} given as arguments.

Stratego compiler is a self-hosting compiler, meaning that the Stratego meta-language is defined in the meta-language itself. A first version of Stratego was written in SML, which was then re-used to compile a further iteration written in Stratego. Stratego compiles programs to C, where the code generation transformations were expressed in Stratego itself.

\subsubsection{Rascal}
Rascal \cite{bos2011rascal, klint2009rascal} is a meta-language born with the main goal of simplifying the task of implementing soure-code analysis and transformation tools. It has also been used to develop domain-specific languages as show in \cite{BASTEN20157}. It features a static type system that integrates both the analysis and transformation domain and where nodes of the AST are fully typed. The type system of Rascal is quite rich: it features functions as first-class values \cite{scott2009programming} in both defined and anonymous forms. It supports parametric polymorphism, algebraic data types, and several built-in data structures such as sets, lists, and relationships.

As said above, AST nodes are typed as well, and can be implemented through algebraic data types. For example, we could define a statement as follows:

\begin{lstlisting}
data Statement =
| Assignment(Id name, Expr expression)
| If(Expr condition, list [Statement] _then, 
    list [Statement] _else)
| While (Expr condition, list [Statement] body)
\end{lstlisting}

\noindent
Notice that it is possible to give names to the single components of algebraic data types.
In Rascal it is possible to define the syntax of the language in the meta-language itself. The syntax is defined by grammar productions that are identified by the keyword \texttt{syntax}. The main difference with traditional parser generators, such as \texttt{Yacc}, is that, in each production we add constructor names to algebraic data types to link the syntax definition to the corresponding node in the AST that will be generated. For instance, the production for the example above can be defined as follows:

\begin{lstlisting}
syntax Statement =
| Assignment: Id var ":=" Expression expression
| If: "if" Expression condition "then" 
    {Statement ";"}* _then  "else" {Statement ";"}* _else "end"
| While: "while" Expression condition "do" {Statement ";"}* "end"
\end{lstlisting}

\noindent
In Rascal it is possible to generate the target code by writing functions that use string templates to format the target code for a high-level programming language, as shown in \cite{BASTEN20157}. This phase might be preceded by a code transformation step. Rascal library allows also to compile and run Java byte code directly from Rascal itself.

\section{Differences with Metacasanova}
In this section we describe the main differences between Metacasanova, the meta-compiler described in this thesis work, and the meta-compilers presented in this chapter.

\subsubsection{Differences with META-II}
META-II allows to define the syntax and the steps to perform for the code generation whenever the syntax is matched. Its meta-language is untyped, which means that the meta-compiler does not check for malformed AST nodes. On the the other hand, in Metacasanova the syntactical elements of the meta-program are statically typed. Furthermore, as we will show in Chapter \ref{ch:functors}, the type system of a language implemented in Metacasanova can be embedded in that of its meta-language. In this way the type checker of Casanova can, at the same time, verify the correctness of the meta-program to define a programming language and a program written in it.

\subsubsection{Differences with RML}
RML and Metacasanova use the same formal specification for the meta-program: the operational semantics. RML expresses the syntax of the implemented language as discriminated unions similar to those of CamL. Metacasanova can at the same time provide the option to define syntactical elements as operators with custom ariety, providing better readability. Moreover, RML does not feature a system of higher-kinded modules that allows to embed the type system of a programming language in the type system of the meta-language itself. Finally, the target code of RML is C, while Metacasanova targets C\# so that its generated code can be plugged in popular game engines, such as Monogame or Unity.

\subsubsection{Differences with Stratego}
Stratego meta-language is based on term-rewriting semantics in contrast with Metacasanova, which uses operational semantics. Another main difference is that Stratego is untyped while Metacasanova is typed. Stratego offers the capability of defining the grammar rules of the language, while Metacasanova lacks a way of defining the syntax in terms of grammar productions.

\subsubsection{Differences with Rascal}
Rascal meta-language is oriented towards the imperative paradigm, although it supports immutability as well, while operational semantics grants, by construction, referential transparency. Metacasanova and Rascal both statically type meta-programs. As shown in \cite{BASTEN20157}, the type checking and code generation requires to explicitly define functions that perform the necessary steps. In Metacasanova we provide language abstractions to express the type checking and code generation of a programming language based on operational semantics. This has the benefit of capturing recurring patterns in hard-coded implementations of the compiler at language level. As Stratego, Rascal offers powerful tools to define the syntax of a programming language and its transformation to the AST, a feature that Metacasanova lacks. Moreover, Rascal features abstractions to define code transformations, which Metacasanova lacks as well due to the fact that it has been designed mainly for the development of DSL's and not also for code transformation techniques and code analysis like Rascal.


\section{Summary}
In this chapter we presented the fundamental topics necessary to understand this thesis work. We started by defining the general architecture of a compiler and then we proceeded to show how to implement its single components. We explained how to use regular expressions to define the syntactical elements of a language and how to build a lexer for them. We explained how grammars are described and how to implement a Parser with different techniques (Parser generators based on LR(k) grammars and Monadic Parsers). We explained how to define a type system for a language and how to express its operational semantics. We concluded by presenting examples of existing meta-compilers and we compared them to Metacasanova, the meta-compiler that we describe in this thesis work. In the next chapter we present the detailed architecture of Metacasanova.