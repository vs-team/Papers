%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% intro.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

List processing libraries are ubiquitous [] in functional languages. Virtually any functional program will employ either lists, sets, maps, lazy sequences or some variations and with those the accompanying higher-order-functions such as map, filter, reduce, etc. []

These libraries allow for shorter, cleaner and more readable code []. Unfortunately, imperative code which ``condensates" various chained operations can very easily be turned into a faster equivalent than its functional counterpart, even though this comes at the cost of conciseness and readability []. This leads to a very undesirable dicotomy between good code and fast code []: no such choice should ever exist!

In this paper we discuss how list processing libraries can be augmented with static information about the shape of the chain of operations performed on the original data source. We use these augmentations to build a static optimizer capable of automatically turning chains of list processing operations into faster equivalents.

To achieve this result we will use the most historically renowned [] set of knowledge in the field of accessing and transforming data: relational algebra. We observe the strict correspondence between most list processing higher order functions and well-known, studied and understood relational operators.

Our technique can be described as follows: list processing operators do not simply return lists; rather, list processing operators return values that can be converted into lists but which type can be used to describe exactly what kind of operation will be performed on the source data. Static operators will then build appropriate optimization and execution functions specifically tailored at compile time for the shape of each operation.

We will use a Haskell-like syntax for describing, optimizing and executing queries. Overlapping instances are so common in our case that we do not claim that our code is actually Haskell, if not for the apparently identical syntax when describing type classes. The semantics of our ``imaginary" compiler and runtime are best described in [Mark Jones]: type classes are logical clauses that are resolved at compile-time through back-tracking. This makes our type system capable of computing essentially any transformation that we may wish for on our code.

We argue that this kind of use of types and metaprogramming is the future of many important compiler optimizations. Until now compiler optimizations have been monolithically embedded in compilers and have offered very little customization opportunities (most of the time just a few compiler switches are offered: hardly any customization at all). Thanks to the emergence of powerful metaprogramming mechanism such as type classes (with overlapping instances handled gracefully) we can finally start building libraries that can be executed completely at compile-time and which realize lots of poweful optimizations, from memory management and reference counting [] to algorithmic optimizations such as the one presented in this paper and even up to analysis of one's source code with abstract interpretation and similar approaches.
